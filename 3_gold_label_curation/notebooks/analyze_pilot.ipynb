{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df130530",
   "metadata": {},
   "source": [
    "# ACSES Pilot Study Analysis\n",
    "\n",
    "This notebook analyzes the results from the pilot study to compare different LLM models for KBLI code validation.\n",
    "\n",
    "## Key Questions to Answer:\n",
    "1. **Success Rate**: For each model, what percentage of calls returned valid JSON vs. errors?\n",
    "2. **Reasoning Quality**: Manual inspection of reasoning strings from each model\n",
    "3. **Confidence Calibration**: Does confidence_score align with difficulty?\n",
    "4. **Consensus**: How often did the 3 runs agree on the is_correct value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da777c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781148ae",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "        def load_pilot_results(filename: str) -> pd.DataFrame:\n",
    "            \"\"\"Load pilot study results from JSON or JSONL file.\"\"\"\n",
    "            # Try JSONL first (new format), then JSON (legacy format)\n",
    "            jsonl_filename = filename.replace('.json', '.jsonl')\n",
    "            json_filepath = os.path.join('..', 'data', 'output', filename)\n",
    "            jsonl_filepath = os.path.join('..', 'data', 'output', jsonl_filename)\n",
    "            \n",
    "            # Check which files exist\n",
    "            jsonl_exists = os.path.exists(jsonl_filepath)\n",
    "            json_exists = os.path.exists(json_filepath)\n",
    "            \n",
    "            if jsonl_exists:\n",
    "                # Load JSONL format (preferred)\n",
    "                print(f\"üìÇ Loading JSONL format: {jsonl_filename}\")\n",
    "                data = []\n",
    "                try:\n",
    "                    with open(jsonl_filepath, 'r', encoding='utf-8') as f:\n",
    "                        for line_num, line in enumerate(f, 1):\n",
    "                            line = line.strip()\n",
    "                            if line:\n",
    "                                try:\n",
    "                                    result = json.loads(line)\n",
    "                                    data.append(result)\n",
    "                                except json.JSONDecodeError as e:\n",
    "                                    print(f\"‚ö†Ô∏è  Warning: Invalid JSON on line {line_num}: {e}\")\n",
    "                                    continue\n",
    "                    \n",
    "                    df = pd.DataFrame(data)\n",
    "                    print(f\"‚úÖ Loaded {len(df)} results from {jsonl_filename}\")\n",
    "                    return df\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading JSONL file: {e}\")\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            elif json_exists:\n",
    "                # Load legacy JSON format\n",
    "                print(f\"üìÇ Loading legacy JSON format: {filename}\")\n",
    "                try:\n",
    "                    with open(json_filepath, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    df = pd.DataFrame(data)\n",
    "                    print(f\"‚úÖ Loaded {len(df)} results from {filename}\")\n",
    "                    return df\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading JSON file: {e}\")\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            else:\n",
    "                # No files found\n",
    "                print(f\"‚ö†Ô∏è  File not found: {filename} or {jsonl_filename}\")\n",
    "                print(\"Available files in output directory:\")\n",
    "                output_dir = os.path.join('..', 'data', 'output')\n",
    "                if os.path.exists(output_dir):\n",
    "                    for f in os.listdir(output_dir):\n",
    "                        if f.endswith(('.json', '.jsonl')):\n",
    "                            print(f\"  - {f}\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Load results for different models\n",
    "        # Update these filenames based on what you've actually generated\n",
    "        flash_results = load_pilot_results('pilot_results_gemini_1_5_flash_latest.json')\n",
    "        # pro_results = load_pilot_results('pilot_results_gemini_1_5_pro_latest.json')  # Uncomment when available\n",
    "        \n",
    "        # Display basic info\n",
    "        if not flash_results.empty:\n",
    "            print(f\"\\nFlash results shape: {flash_results.shape}\")\n",
    "            print(f\"Columns: {list(flash_results.columns)}\")\n",
    "            print(f\"Date range: {flash_results['timestamp'].min()} to {flash_results['timestamp'].max()}\")\n",
    "# Load results for different models\n",
    "# Update these filenames based on what you've actually generated\n",
    "flash_results = load_pilot_results('pilot_results_gemini_1_5_flash_latest.json')\n",
    "# pro_results = load_pilot_results('pilot_results_gemini_1_5_pro_latest.json')  # Uncomment when available\n",
    "\n",
    "# Display basic info\n",
    "if not flash_results.empty:\n",
    "    print(f\"\\nFlash results shape: {flash_results.shape}\")\n",
    "    print(f\"Columns: {list(flash_results.columns)}\")\n",
    "    print(f\"Date range: {flash_results['timestamp'].min()} to {flash_results['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6496fca",
   "metadata": {},
   "source": [
    "## 2. Success Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb864dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_success_rate(df: pd.DataFrame, model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze success rate and error patterns.\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    total_calls = len(df)\n",
    "    successful_calls = df['success'].sum()\n",
    "    success_rate = successful_calls / total_calls\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = df[df['success'] == False]\n",
    "    error_types = errors['error_type'].value_counts() if not errors.empty else pd.Series()\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'total_calls': total_calls,\n",
    "        'successful_calls': successful_calls,\n",
    "        'success_rate': success_rate,\n",
    "        'error_count': len(errors),\n",
    "        'error_types': error_types.to_dict() if not error_types.empty else {}\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze success rates\n",
    "flash_success = analyze_success_rate(flash_results, 'Gemini 1.5 Flash')\n",
    "# pro_success = analyze_success_rate(pro_results, 'Gemini 1.5 Pro')  # Uncomment when available\n",
    "\n",
    "print(\"üìä SUCCESS RATE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if flash_success:\n",
    "    print(f\"\\n{flash_success['model']}:\")\n",
    "    print(f\"  Total API calls: {flash_success['total_calls']}\")\n",
    "    print(f\"  Successful calls: {flash_success['successful_calls']}\")\n",
    "    print(f\"  Success rate: {flash_success['success_rate']:.1%}\")\n",
    "    if flash_success['error_types']:\n",
    "        print(f\"  Error types: {flash_success['error_types']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize success rates\n",
    "if not flash_results.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Success rate bar chart\n",
    "    models = ['Gemini 1.5 Flash']  # Add more models when available\n",
    "    success_rates = [flash_success['success_rate']]\n",
    "    \n",
    "    axes[0].bar(models, success_rates, color=['skyblue'])\n",
    "    axes[0].set_ylabel('Success Rate')\n",
    "    axes[0].set_title('API Call Success Rate by Model')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, rate in enumerate(success_rates):\n",
    "        axes[0].text(i, rate + 0.01, f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # Processing time distribution for successful calls\n",
    "    successful_flash = flash_results[flash_results['success'] == True]\n",
    "    if not successful_flash.empty and 'processing_time_seconds' in successful_flash.columns:\n",
    "        axes[1].hist(successful_flash['processing_time_seconds'], bins=20, alpha=0.7, \n",
    "                    label='Flash', color='skyblue')\n",
    "        axes[1].set_xlabel('Processing Time (seconds)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Processing Time Distribution')\n",
    "        axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1d7a4",
   "metadata": {},
   "source": [
    "## 3. Reasoning Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133eb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reasoning_analysis(df: pd.DataFrame, model_name: str, n_samples: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Sample reasoning strings for manual inspection.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    successful = df[df['success'] == True].copy()\n",
    "    if successful.empty:\n",
    "        print(f\"No successful results found for {model_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Sample diverse cases\n",
    "    sample_df = successful.sample(min(n_samples, len(successful)), random_state=42)\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis_cols = ['sample_id', 'assigned_kbli_code', 'is_correct', 'confidence_score', \n",
    "                     'original_text', 'reasoning']\n",
    "    \n",
    "    available_cols = [col for col in analysis_cols if col in sample_df.columns]\n",
    "    sample_analysis = sample_df[available_cols].copy()\n",
    "    \n",
    "    # Add reasoning length and quality metrics\n",
    "    if 'reasoning' in sample_analysis.columns:\n",
    "        sample_analysis['reasoning_length'] = sample_analysis['reasoning'].astype(str).str.len()\n",
    "        sample_analysis['mentions_hierarchy'] = sample_analysis['reasoning'].astype(str).str.contains(\n",
    "            'section|division|group|class|sub-class', case=False, na=False\n",
    "        )\n",
    "    \n",
    "    return sample_analysis\n",
    "\n",
    "# Sample reasoning for manual inspection\n",
    "print(\"üîç REASONING QUALITY SAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not flash_results.empty:\n",
    "    flash_reasoning = sample_reasoning_analysis(flash_results, 'Flash', 15)\n",
    "    \n",
    "    if not flash_reasoning.empty:\n",
    "        print(f\"\\nüìù Sample reasoning from Gemini 1.5 Flash:\")\n",
    "        print(f\"Average reasoning length: {flash_reasoning['reasoning_length'].mean():.0f} characters\")\n",
    "        print(f\"Mentions hierarchy: {flash_reasoning['mentions_hierarchy'].mean():.1%}\")\n",
    "        \n",
    "        # Display first few examples\n",
    "        display(flash_reasoning[['assigned_kbli_code', 'is_correct', 'confidence_score', \n",
    "                                'reasoning_length', 'mentions_hierarchy']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed reasoning examples\n",
    "if not flash_results.empty and not flash_reasoning.empty:\n",
    "    print(\"üìñ DETAILED REASONING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(flash_reasoning.head(3).iterrows()):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"KBLI Code: {row.get('assigned_kbli_code', 'N/A')}\")\n",
    "        print(f\"Prediction: {'Correct' if row.get('is_correct') else 'Incorrect'}\")\n",
    "        print(f\"Confidence: {row.get('confidence_score', 'N/A')}\")\n",
    "        print(f\"Job Description: {row.get('original_text', 'N/A')[:100]}...\")\n",
    "        print(f\"Reasoning: {row.get('reasoning', 'N/A')[:300]}...\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b86667",
   "metadata": {},
   "source": [
    "## 4. Confidence Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_calibration(df: pd.DataFrame, model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze how well confidence scores align with correctness.\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    successful = df[df['success'] == True].copy()\n",
    "    if successful.empty or 'confidence_score' not in successful.columns:\n",
    "        return {}\n",
    "    \n",
    "    # Remove any non-numeric confidence scores\n",
    "    successful = successful[pd.to_numeric(successful['confidence_score'], errors='coerce').notna()]\n",
    "    successful['confidence_score'] = pd.to_numeric(successful['confidence_score'])\n",
    "    \n",
    "    if successful.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    conf_stats = {\n",
    "        'model': model_name,\n",
    "        'mean_confidence': successful['confidence_score'].mean(),\n",
    "        'std_confidence': successful['confidence_score'].std(),\n",
    "        'min_confidence': successful['confidence_score'].min(),\n",
    "        'max_confidence': successful['confidence_score'].max()\n",
    "    }\n",
    "    \n",
    "    # Confidence by correctness\n",
    "    if 'is_correct' in successful.columns:\n",
    "        correct_conf = successful[successful['is_correct'] == True]['confidence_score']\n",
    "        incorrect_conf = successful[successful['is_correct'] == False]['confidence_score']\n",
    "        \n",
    "        conf_stats.update({\n",
    "            'mean_conf_correct': correct_conf.mean() if not correct_conf.empty else None,\n",
    "            'mean_conf_incorrect': incorrect_conf.mean() if not incorrect_conf.empty else None,\n",
    "            'conf_difference': (correct_conf.mean() - incorrect_conf.mean()) if not correct_conf.empty and not incorrect_conf.empty else None\n",
    "        })\n",
    "    \n",
    "    return conf_stats\n",
    "\n",
    "# Analyze confidence calibration\n",
    "flash_conf = analyze_confidence_calibration(flash_results, 'Flash')\n",
    "\n",
    "print(\"üéØ CONFIDENCE CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if flash_conf:\n",
    "    print(f\"\\n{flash_conf['model']} Model:\")\n",
    "    print(f\"  Mean confidence: {flash_conf['mean_confidence']:.3f}\")\n",
    "    print(f\"  Std confidence: {flash_conf['std_confidence']:.3f}\")\n",
    "    print(f\"  Range: {flash_conf['min_confidence']:.3f} - {flash_conf['max_confidence']:.3f}\")\n",
    "    \n",
    "    if flash_conf.get('mean_conf_correct') is not None:\n",
    "        print(f\"  Mean confidence (correct): {flash_conf['mean_conf_correct']:.3f}\")\n",
    "        print(f\"  Mean confidence (incorrect): {flash_conf['mean_conf_incorrect']:.3f}\")\n",
    "        print(f\"  Difference: {flash_conf['conf_difference']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence calibration\n",
    "if not flash_results.empty:\n",
    "    successful_flash = flash_results[flash_results['success'] == True].copy()\n",
    "    \n",
    "    if not successful_flash.empty and 'confidence_score' in successful_flash.columns:\n",
    "        # Convert confidence to numeric\n",
    "        successful_flash['confidence_score'] = pd.to_numeric(\n",
    "            successful_flash['confidence_score'], errors='coerce'\n",
    "        )\n",
    "        successful_flash = successful_flash[successful_flash['confidence_score'].notna()]\n",
    "        \n",
    "        if not successful_flash.empty:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Confidence distribution\n",
    "            axes[0].hist(successful_flash['confidence_score'], bins=20, alpha=0.7, \n",
    "                        color='skyblue', edgecolor='black')\n",
    "            axes[0].set_xlabel('Confidence Score')\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "            axes[0].set_title('Confidence Score Distribution')\n",
    "            axes[0].axvline(successful_flash['confidence_score'].mean(), color='red', \n",
    "                           linestyle='--', label=f'Mean: {successful_flash[\"confidence_score\"].mean():.3f}')\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # Confidence by correctness (if available)\n",
    "            if 'is_correct' in successful_flash.columns:\n",
    "                correct_conf = successful_flash[successful_flash['is_correct'] == True]['confidence_score']\n",
    "                incorrect_conf = successful_flash[successful_flash['is_correct'] == False]['confidence_score']\n",
    "                \n",
    "                if not correct_conf.empty and not incorrect_conf.empty:\n",
    "                    axes[1].hist([correct_conf, incorrect_conf], bins=15, alpha=0.7, \n",
    "                                label=['Correct', 'Incorrect'], color=['green', 'red'])\n",
    "                    axes[1].set_xlabel('Confidence Score')\n",
    "                    axes[1].set_ylabel('Frequency')\n",
    "                    axes[1].set_title('Confidence by Correctness')\n",
    "                    axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539c363",
   "metadata": {},
   "source": [
    "## 5. Consensus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_consensus(df: pd.DataFrame, model_name: str, n_runs: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze how often multiple runs agree on the is_correct value.\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    successful = df[df['success'] == True].copy()\n",
    "    if successful.empty or 'sample_id' not in successful.columns:\n",
    "        return {}\n",
    "    \n",
    "    # Group by sample_id to analyze consensus\n",
    "    consensus_data = []\n",
    "    \n",
    "    for sample_id in successful['sample_id'].unique():\n",
    "        sample_runs = successful[successful['sample_id'] == sample_id]\n",
    "        \n",
    "        if len(sample_runs) < n_runs:\n",
    "            continue  # Skip incomplete samples\n",
    "        \n",
    "        # Get is_correct values for this sample\n",
    "        if 'is_correct' in sample_runs.columns:\n",
    "            is_correct_values = sample_runs['is_correct'].tolist()\n",
    "            \n",
    "            # Calculate consensus\n",
    "            correct_count = sum(is_correct_values)\n",
    "            total_count = len(is_correct_values)\n",
    "            \n",
    "            consensus_type = None\n",
    "            if correct_count == total_count:\n",
    "                consensus_type = 'unanimous_correct'\n",
    "            elif correct_count == 0:\n",
    "                consensus_type = 'unanimous_incorrect'\n",
    "            elif correct_count > total_count / 2:\n",
    "                consensus_type = 'majority_correct'\n",
    "            else:\n",
    "                consensus_type = 'majority_incorrect'\n",
    "            \n",
    "            consensus_data.append({\n",
    "                'sample_id': sample_id,\n",
    "                'assigned_kbli_code': sample_runs.iloc[0].get('assigned_kbli_code', 'N/A'),\n",
    "                'correct_count': correct_count,\n",
    "                'total_count': total_count,\n",
    "                'consensus_type': consensus_type,\n",
    "                'mean_confidence': sample_runs['confidence_score'].mean() if 'confidence_score' in sample_runs.columns else None\n",
    "            })\n",
    "    \n",
    "    if not consensus_data:\n",
    "        return {'model': model_name, 'no_data': True}\n",
    "    \n",
    "    consensus_df = pd.DataFrame(consensus_data)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    consensus_stats = {\n",
    "        'model': model_name,\n",
    "        'total_samples': len(consensus_df),\n",
    "        'consensus_breakdown': consensus_df['consensus_type'].value_counts().to_dict(),\n",
    "        'unanimous_rate': len(consensus_df[consensus_df['consensus_type'].str.contains('unanimous')]) / len(consensus_df),\n",
    "        'correct_rate': len(consensus_df[consensus_df['consensus_type'].str.contains('correct')]) / len(consensus_df)\n",
    "    }\n",
    "    \n",
    "    return consensus_stats, consensus_df\n",
    "\n",
    "# Analyze consensus\n",
    "print(\"ü§ù CONSENSUS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not flash_results.empty:\n",
    "    flash_consensus = analyze_consensus(flash_results, 'Flash')\n",
    "    \n",
    "    if isinstance(flash_consensus, tuple):\n",
    "        flash_stats, flash_consensus_df = flash_consensus\n",
    "        \n",
    "        print(f\"\\n{flash_stats['model']} Model:\")\n",
    "        print(f\"  Total samples analyzed: {flash_stats['total_samples']}\")\n",
    "        print(f\"  Unanimous agreement rate: {flash_stats['unanimous_rate']:.1%}\")\n",
    "        print(f\"  Overall correct rate: {flash_stats['correct_rate']:.1%}\")\n",
    "        print(f\"  Consensus breakdown:\")\n",
    "        for consensus_type, count in flash_stats['consensus_breakdown'].items():\n",
    "            print(f\"    {consensus_type}: {count} ({count/flash_stats['total_samples']:.1%})\")\n",
    "    \n",
    "    elif flash_consensus.get('no_data'):\n",
    "        print(f\"\\n{flash_consensus['model']} Model: No consensus data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consensus patterns\n",
    "if not flash_results.empty and 'flash_consensus_df' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Consensus type distribution\n",
    "    consensus_counts = flash_consensus_df['consensus_type'].value_counts()\n",
    "    colors = {'unanimous_correct': 'green', 'unanimous_incorrect': 'red', \n",
    "             'majority_correct': 'lightgreen', 'majority_incorrect': 'lightcoral'}\n",
    "    \n",
    "    bar_colors = [colors.get(ct, 'gray') for ct in consensus_counts.index]\n",
    "    axes[0].bar(range(len(consensus_counts)), consensus_counts.values, color=bar_colors)\n",
    "    axes[0].set_xticks(range(len(consensus_counts)))\n",
    "    axes[0].set_xticklabels(consensus_counts.index, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "    axes[0].set_title('Consensus Type Distribution')\n",
    "    \n",
    "    # Confidence by consensus type\n",
    "    if 'mean_confidence' in flash_consensus_df.columns and flash_consensus_df['mean_confidence'].notna().any():\n",
    "        consensus_conf = flash_consensus_df.groupby('consensus_type')['mean_confidence'].mean()\n",
    "        \n",
    "        bar_colors_conf = [colors.get(ct, 'gray') for ct in consensus_conf.index]\n",
    "        axes[1].bar(range(len(consensus_conf)), consensus_conf.values, color=bar_colors_conf)\n",
    "        axes[1].set_xticks(range(len(consensus_conf)))\n",
    "        axes[1].set_xticklabels(consensus_conf.index, rotation=45, ha='right')\n",
    "        axes[1].set_ylabel('Mean Confidence Score')\n",
    "        axes[1].set_title('Mean Confidence by Consensus Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03474fe",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e65f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã PILOT STUDY SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not flash_results.empty:\n",
    "    print(f\"\\nü§ñ GEMINI 1.5 FLASH RESULTS:\")\n",
    "    if flash_success:\n",
    "        print(f\"  ‚úì Success Rate: {flash_success['success_rate']:.1%}\")\n",
    "    if flash_conf:\n",
    "        print(f\"  ‚úì Mean Confidence: {flash_conf['mean_confidence']:.3f}\")\n",
    "        if flash_conf.get('conf_difference'):\n",
    "            print(f\"  ‚úì Confidence Calibration: {flash_conf['conf_difference']:.3f} difference\")\n",
    "    if 'flash_stats' in locals():\n",
    "        print(f\"  ‚úì Unanimous Agreement: {flash_stats['unanimous_rate']:.1%}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"\\n1. MODEL SELECTION:\")\n",
    "if flash_success and flash_success['success_rate'] > 0.9:\n",
    "    print(\"   ‚úÖ Flash shows good API reliability (>90% success rate)\")\n",
    "elif flash_success:\n",
    "    print(f\"   ‚ö†Ô∏è  Flash success rate is {flash_success['success_rate']:.1%} - investigate error patterns\")\n",
    "\n",
    "print(\"\\n2. PROMPT ENGINEERING:\")\n",
    "if 'flash_reasoning' in locals() and not flash_reasoning.empty:\n",
    "    hierarchy_mention_rate = flash_reasoning['mentions_hierarchy'].mean()\n",
    "    if hierarchy_mention_rate > 0.7:\n",
    "        print(\"   ‚úÖ Model effectively uses hierarchical context\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Consider emphasizing hierarchy usage in prompts\")\n",
    "\n",
    "print(\"\\n3. QUALITY CONTROL:\")\n",
    "if 'flash_stats' in locals() and flash_stats['unanimous_rate'] > 0.6:\n",
    "    print(\"   ‚úÖ Good consensus rate - 3 runs provide reliable validation\")\n",
    "elif 'flash_stats' in locals():\n",
    "    print(\"   ‚ö†Ô∏è  Low consensus rate - consider increasing runs or adjusting temperature\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS:\")\n",
    "print(\"   üìù Run comparison with Gemini 1.5 Pro\")\n",
    "print(\"   üìä Expand sample size for full validation\")\n",
    "print(\"   üîß Fine-tune prompts based on error analysis\")\n",
    "print(\"   üéØ Implement confidence-based filtering for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417ee24",
   "metadata": {},
   "source": [
    "## 7. Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics\n",
    "summary_data = {\n",
    "    'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'flash_results': flash_success if 'flash_success' in locals() else {},\n",
    "    'flash_confidence': flash_conf if 'flash_conf' in locals() else {},\n",
    "    'flash_consensus': flash_stats if 'flash_stats' in locals() else {}\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "output_path = os.path.join('..', 'data', 'output', 'pilot_analysis_summary.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"üìä Analysis summary exported to: {output_path}\")\n",
    "\n",
    "# Export detailed consensus data if available\n",
    "if 'flash_consensus_df' in locals():\n",
    "    consensus_path = os.path.join('..', 'data', 'output', 'consensus_analysis.csv')\n",
    "    flash_consensus_df.to_csv(consensus_path, index=False)\n",
    "    print(f\"üìä Consensus analysis exported to: {consensus_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete! Ready for next phase of the pilot study.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
