# ACSES Pilot Study - Gold Label Curation

This module implements the ACSES (Automated Classification System Evaluation Study) pilot study to determine the best LLM for KBLI code validation using hierarchical prompting.

## High-Level Project Overview

The ACSES pilot study is designed to evaluate and compare different Gemini language models for automated job description classification using the Indonesian Standard Industrial Classification (KBLI) system. This robust, production-ready implementation ensures reliable data curation at scale.

### Key Features

- **Multi-Model Evaluation**: Compare Gemini Flash, Pro, and Flash Lite models
- **Hierarchical Classification**: Uses structured KBLI codebook for accurate classification
- **Robust Resume Capability**: Zero data loss with JSONL format and smart progress tracking
- **Production-Ready**: Handles API limits, network interruptions, and resource exhaustion gracefully
- **Comprehensive Analysis**: Full pipeline from data preparation to performance evaluation

### Research Objectives

1. **Model Performance Comparison**: Determine which Gemini model provides the highest accuracy for KBLI classification
2. **Reliability Assessment**: Evaluate consistency across multiple runs per sample
3. **Scalability Testing**: Validate approach for large-scale data curation
4. **Cost-Effectiveness**: Compare model performance vs. API costs

## Documentation Structure

For detailed information, see the organized documentation:

- **[Installation & Setup](docs/setup.md)**: Environment configuration and API setup
- **[Workflow Guide](docs/workflow.md)**: Step-by-step pipeline execution  
- **[API Reference](docs/api_reference.md)**: Code documentation and technical details

## Quick Start

1. **Setup Environment**: Follow [setup instructions](docs/setup.md)
2. **Run Pipeline**: Follow [workflow guide](docs/workflow.md)
3. **API Reference**: Check [code documentation](docs/api_reference.md)

## Directory Structure

```
3_gold_label_curation/
â”œâ”€â”€ src/                           # ðŸ“š Core library code (reusable modules)
â”‚   â”œâ”€â”€ config.py                  # Centralized configuration management
â”‚   â”œâ”€â”€ utils/                     # Common utilities and helpers
â”‚   â”œâ”€â”€ api/                       # API client implementations
â”‚   â”œâ”€â”€ pipeline/                  # Data processing pipeline components
â”‚   â””â”€â”€ analysis/                  # Analysis and evaluation modules
â”œâ”€â”€ scripts/                       # ï¿½ Executable entry points
â”‚   â”œâ”€â”€ prepare_codebook.py        # Phase 1: Codebook preparation
â”‚   â”œâ”€â”€ add_unique_ids.py          # Phase 2A: Add UUID identifiers
â”‚   â”œâ”€â”€ run_pilot_study.py         # Phase 2B: Execute pilot study
â”‚   â”œâ”€â”€ analyze_results.py         # Analysis and reporting
â”‚   â””â”€â”€ setup_and_validate.py     # Environment validation
â”œâ”€â”€ docs/                          # ï¿½ðŸ“š Consolidated Documentation
â”‚   â”œâ”€â”€ setup.md                   # Installation and environment setup
â”‚   â”œâ”€â”€ workflow.md                # Step-by-step pipeline documentation  
â”‚   â””â”€â”€ api_reference.md           # Code documentation and API reference
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                     # Input datasets and codebooks
â”‚   â””â”€â”€ output/                    # Generated results and analysis
â”œâ”€â”€ notebooks/                     # Jupyter analysis notebooks
â”œâ”€â”€ prompts/                       # LLM prompt templates
â”œâ”€â”€ tests/                         # Unit and integration tests
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This overview file
```

## Implementation Highlights

### ðŸ›¡ï¸ Robust & Resumable Architecture
- **JSONL Format**: Each API result saved immediately, never overwritten
- **Resume Anywhere**: If interrupted, just restart the script
- **Smart Progress Tracking**: Knows exactly which sample+run combinations are complete
- **Resource Exhaustion Safe**: Handles API quota limits gracefully

### ðŸ”„ Production-Ready Features
- **Rate Limiting**: Automatic compliance with API limits
- **Error Handling**: Comprehensive error recovery and logging
- **UUID Tracking**: Unique identifiers for perfect data lineage
- **Multi-Model Support**: Compare different Gemini models systematically

## Key Benefits

### UUID Implementation Benefits
- ðŸ†” **Global Unique IDs**: Each sample has a globally unique identifier
- ðŸ”„ **Robust Resume**: Perfect tracking with UUID+run_number combinations
- ðŸ”— **Data Lineage**: Complete traceability from raw data to results
- ðŸ› **Enhanced Debugging**: Easy identification of problematic samples

### JSONL Architecture Benefits  
- ðŸ’¾ **Zero Data Loss**: Each result saved immediately, never overwritten
- âš¡ **Resume Anywhere**: Interrupted processes continue exactly where they left off
- ðŸŽ¯ **Smart Progress**: Automatic detection of completed sample+run combinations
- ðŸ›¡ï¸ **Error Resilience**: Graceful handling of API limits and network issues

## Expected Outcomes

### Success Metrics
- **Accuracy Rate**: Target >90% for production readiness
- **Consensus Rate**: Target >60% unanimous agreement across runs
- **Reliability**: Consistent performance across different sample types

### Decision Framework
- Model selection based on accuracy, consistency, and cost-effectiveness
- Quality control thresholds derived from confidence calibration
- Scalability validation for full dataset processing

## Contact & Support

For detailed implementation guidance, refer to the comprehensive documentation in the `docs/` folder or the analysis notebooks for results interpretation.
